## Deep Learning book by Nikolenko

Notebooks from Colab
* [Chapter 2: keras](DL_chapter_2_keras.ipynb)
* [Chapter 2: tf](DL_chapter_2_tf.ipynb)
* [Chapter 3: mnist](DL_chapter_3_mnist.ipynb)
* [Chapter 4: keras for initialization](DL_chapter_4_keras_for_initialization.ipynb)
* [Chapter 4: minibatch normalization](DL_chapter_4_minibatch_normalization.ipynb)
* [Chapter 5: autoencoders for mnist](DL_chapter_5_autoencoders_for_mnist.ipynb)
* [Chapter 5: basic convolutions](DL_chapter_5_basic_convolutions.ipynb)
* [Chapter 6: char-level generation with RNN](DL_chapter_6_char_lever_generation_with_RNN.ipynb)
* [Chapter 7: word2vec](DL_chapter_7_word2vec.ipynb)
* [Chapter 7: word2vec (local, im-memory)](DL_chapter_7_word2vec_local_in_memory.ipynb)
* [Chapter 7: word2vec (local, im-memory, load)](DL_chapter_7_word2vec_local_in_memory_load.ipynb)
* [Chapter 8: generative-adversarial network (GAN)](DL_chapter_8_GAN.ipynb)
* [Chapter 8: adversarial auto-encoder (AAE)](DL_chapter_8_AAE.ipynb)


[Scans of my notes](notes)

Other links:
* [Convolutions arithmetic](https://github.com/vdumoulin/conv_arithmetic)
* [Andrej Karpathy about RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/): very impressive examples, funny quotes:
  * `RNNs are neural networks and everything works monotonically better (if done right) if you put on your deep learning hat and start stacking models up like pancakes.`
  * `In case you were wondering, the yahoo url above doesnâ€™t actually exist, the model just hallucinated it.`
* [vc.ru about app for shot detection](https://vc.ru/tribuna/92856-kak-neyroseti-pomogayut-razvivat-navyki-po-strelbe-istoriya-ot-chempionov-mira). Nice technique for augmentation: add podcasts and bloggers to initial audio track
